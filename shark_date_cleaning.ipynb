{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Sharktale, SharkTrack\n",
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xlrd #!pip install country_converter, keep this here pls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import of Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Excel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define path to xls\n",
    "url = 'https://www.sharkattackfile.net/spreadsheets/GSAF5.xls'\n",
    "df = pd.read_excel(url)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data at a Glance \n",
    "Reviewing how the database is organised by using .head(), .info(), .describe(), .tail(), .columns. This will help me on how to pepare the data cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns Cleaning and Renaming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have reviewed the columns and decided to do the following: \n",
    "- Remove extra \" \" and also replacing extra \" \" with \"_\". \n",
    "- Dropping the following columns: \"Case_Number_1\", \"Unnamed_21\". \"Unnamed_22\" . \n",
    "- Renaming:  'Fatal Y/N': 'Fatal', 'Sex': 'Gender',  'pdf': 'PDF',  and  'Species ': 'Species', 'href': 'Link','original order': 'Original_Order', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns= {\n",
    "    'Fatal Y/N': 'Fatal',\n",
    "    'Species ': 'Species',\n",
    "    'pdf': 'PDF', \n",
    "    'Sex': 'Gender',\n",
    "    'href formula': 'Href_formula', \n",
    "    'href': 'Link',\n",
    "    'Case Number': 'Case_Number', \n",
    "    'Case Number.1': 'Case_Number_1',\n",
    "    'original order': 'Original_Order', \n",
    "    'Unnamed: 21': 'Unnamed_21', \n",
    "    'Unnamed: 22': 'Unnamed_22'\n",
    "})\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Case_Number\"].isin(df[\"Case_Number_1\"]).value_counts() # There is less than 10% difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Columns\n",
    "df = df.drop(columns=[\"Unnamed_21\", \"Unnamed_22\", \"PDF\", \"Href_formula\", \"Case_Number_1\"])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cleaning and replacing all Nan's of Columns that are objects/strings\n",
    "df[\"Country\"] = df[\"Country\"].fillna(\"Unknown\").str.strip().astype(str)\n",
    "df[\"State\"] = df[\"State\"].fillna(\"Unknown\").str.strip().astype(str)\n",
    "df[\"Location\"] = df[\"Location\"].fillna(\"Unknown\").str.strip().astype(str)\n",
    "df[\"Activity\"] = df[\"Activity\"].fillna(\"Unknown\").str.strip().astype(str)\n",
    "df[\"Name\"] = df[\"Name\"].fillna(\"Unknown\").str.strip().astype(str)\n",
    "df[\"Gender\"] = df[\"Gender\"].fillna(\"Unknown\").str.strip().astype(str)\n",
    "df[\"Injury\"] = df[\"Injury\"].fillna(\"Unknown\").str.strip().astype(str)\n",
    "df[\"Species\"] = df[\"Species\"].fillna(\"Unknown\").str.strip().astype(str)\n",
    "df[\"Fatal\"] = df[\"Fatal\"].fillna(\"Unknown\").str.strip().astype(str)\n",
    "df[\"Link\"] = df[\"Link\"].fillna(\"Unknown\").str.strip().astype(str)\n",
    "df[\"Source\"] = df[\"Source\"].fillna(\"Unknown\").str.strip().astype(str)\n",
    "df[\"Type\"] = df[\"Type\"].fillna(\"Unknown\").str.strip().astype(str)\n",
    "df[\"Name\"] = df[\"Name\"].fillna(\"Unknown\").str.strip().astype(str)\n",
    "#df[\"Age\"] = df[\"Age\"].fillna(\"Unknown\").str.strip().astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum())  # Check-up for all Nan's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Column 'Month'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_month(date): \n",
    "    months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "    #If it founds the info of a given month, it returns an output of this cleaning function\n",
    "    try:\n",
    "        for month in months: \n",
    "            if re.search(month, date):\n",
    "                return month\n",
    "    except TypeError:\n",
    "        pass \n",
    "  \n",
    "df[\"Month\"] = df[\"Date\"]\n",
    "df[\"Month\"] = df[\"Date\"].apply(finding_month)\n",
    "df[\"Month\"].fillna(value=\"Unknown\", inplace=True)\n",
    "print(df[\"Month\"].value_counts())  # Check unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Column 'Date'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reviewing the column 'Date'. I am stripping the time component, because we already have a column with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"].unique()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Date\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure all values are strings before applying the function \n",
    "df[\"Date\"] = df[\"Date\"].astype(str).str.lower().str.strip() # Converting everything to lowercase, and removing extra spaces before and after text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(text):\n",
    "    \"\"\"\n",
    "    Extracts valid date-like information from messy strings.\n",
    "    Converts multiple formats into a standard date format (DD-MM-YYYY).\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text.lower().strip() in [\"none\", \"nan\", \"null\", \"\"]:\n",
    "        return None  # Handle missing values\n",
    "\n",
    "    text = text.strip()  # Remove leading/trailing spaces\n",
    "\n",
    "    # Define regex patterns to capture different date formats\n",
    "    date_patterns = [\n",
    "        r\"\\d{4}-\\d{2}-\\d{2}\",             # \"2023-05-20\" (ISO format)\n",
    "        r\"\\d{1,2}-[a-z]{3}-\\d{4}\",        # \"18-may-2023\"\n",
    "        r\"\\d{1,2} [a-z]{3}-\\d{4}\",        # \"09 may-2023\"\n",
    "        r\"\\d{1,2} [a-z]{3} \\d{4}\",        # \"15 Mar 2024\"\n",
    "        r\"\\d{1,2}-[A-Za-z]{3}-\\d{4}\",     # \"23-Jun-2023\"\n",
    "        r\"\\d{1,2} [A-Za-z]{3} \\d{4}\",     # \"15 Mar 2024\"\n",
    "        r\"\\d{1,2} [A-Za-z]+ \\d{4}\",       # \"15 March 2024\"\n",
    "        r\"\\d{1,2}(st|nd|rd|th)? of [A-Za-z]+, \\d{4}\"  # \"24th of May, 2022\"\n",
    "    ]\n",
    "\n",
    "    for pattern in date_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(0)  # Extract the matched date part\n",
    "\n",
    "    return None  # Return None if no valid date is found\n",
    "\n",
    " # Clean the \"Date\" column before applying regex\n",
    "df[\"Date\"] = df[\"Date\"].astype(str).str.strip()\n",
    "\n",
    "# Apply extraction function\n",
    "df[\"Cleaned_Date\"] = df[\"Date\"].apply(extract_date)\n",
    "\n",
    "# Convert extracted dates into proper datetime format\n",
    "df[\"Cleaned_Date\"] = pd.to_datetime(df[\"Cleaned_Date\"], errors=\"coerce\")\n",
    "\n",
    "# Convert to DD-MM-YYYY format (Final Step)\n",
    "df[\"Cleaned_Date\"] = df[\"Cleaned_Date\"].dt.strftime('%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values (NaN)\n",
    "null_count = df[\"Cleaned_Date\"].isna().sum()\n",
    "print(f\"Number of null values: {null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show only rows where Cleaned_Date is NaN\n",
    "missing_rows = df[df[\"Cleaned_Date\"].isna()]\n",
    "missing_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = df.groupby(\"Year\")[\"Date\"].count()\n",
    "df_count[df_count.index > 1750].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting Datetime to Date format "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning for the Column 'Type' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Type.unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Type\"].value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Type\"].value_counts(dropna=False)) # I wanted to review the Nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviewing how many were provoked and unprovoked \n",
    "print(df[\"Type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Type\"] = df[\"Type\"].str.strip()\n",
    "df[\"Type\"] = df[\"Type\"].replace({\n",
    "    \" Provoked\": \"Provoked\", \n",
    "    \"Unconfirmed\": \"Unknown\",\n",
    "    \"?\" : \"Unknown\",\n",
    "    \"Invalid\": \"Unknown\",\n",
    "    \"Under investigation\": \"Unknown\",\n",
    "    \"Questionable\": \"Unknown\", \n",
    "    \"Watercraft\" : \"Water Vehicle\", \n",
    "    \"Boat\" : \"Water Vehicle\"\n",
    "})\n",
    "\n",
    "df[\"Type\"] = df[\"Type\"].fillna(\"Unknown\")\n",
    "print(df[\"Type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for Unprovoked Shark Attacks \n",
    "filter_provoked_gen = ((df[\"Type\"] == \"Unprovoked\") & (df[\"Year\"] >= 1960) & (df[\"Year\"]<2025))\n",
    "df = df.loc[filter_provoked_gen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning for the Column 'Gender'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Gender\"] = df[\"Gender\"].replace({\n",
    "    \" M\": \"M\", \n",
    "    \"M \": \"M\", \n",
    "    \"M x 2\": \"M\", \n",
    "    \" nan\": \"Unknown\", \n",
    "    \"lli\": \"Unknown\", \n",
    "    \"N\": \"M\", \n",
    "    \".\": \"Unknown\"\n",
    "    })\n",
    "df[\"Gender\"] = df[\"Gender\"].fillna(\"Unknown\")\n",
    "df.Gender.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning for the Column 'Species'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Species\"].unique()\n",
    "print(df[\"Species\"].unique()[:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the Column 'Species' a bit \n",
    "df[\"Species\"] = df[\"Species\"].str.extract(r'([A-Za-z\\s-]+)').fillna(\"Unknown\") # Cleaning here the unnecessary details\n",
    "df[\"Species\"] = df[\"Species\"].replace({\n",
    "    \"Not stated\":\"Unknown\"\n",
    "})\n",
    "print(df[\"Species\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Species_Types\"] = df[\"Species\"].copy() # creating a copy of species and naming it Species_Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Species_Types\"] = df[\"Species_Types\"].astype(str).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Species_Types\"] = df[\"Species_Types\"].fillna(\"Unknown\").astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Species_Types\"] = df[\"Species_Types\"].replace({\n",
    "    \"Not stated\": \"Unknown\",\n",
    "    \"Not specified\": \"Unknown\",\n",
    "    \"Invalid\": \"Unknown\",\n",
    "    \"Great White Shark\": \"White Shark\"   \n",
    "}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_shark_name(species): \n",
    "    species = species.strip().lower()\n",
    "\n",
    "    match = re.search(r\"(?:\\b\\w+\\s+)*?(great white|spinner|leopard|whitetip|sandtiger|lemon|horn|white|tiger|broadnos|bull|Bull|hammerhead|hammer|grey|Grey|Hammerhead|blacktip|Blacktip|whale|nurse|mako|reef|Reef)(?:\\s+\\w+)*?\\s*shark(?:\\s+\\w+)*?\", species)\n",
    "\n",
    "    if match:\n",
    "        #return match.group(0).title()\n",
    "        return match.group(1).title() + \" Shark\"\n",
    "    else: \n",
    "        return \"Other/Unknown\"\n",
    "\n",
    "df[\"Species_Types\"] = df[\"Species_Types\"].apply(clean_shark_name)\n",
    "\n",
    "print(df[\"Species_Types\"].value_counts())  # Check unique values\n",
    "print(df[\"Species_Types\"].tail())  # Display first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Species_Types\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"Location\"] = df[\"Location\"].astype(str).str.strip()\n",
    "print(df[\"Species_Types\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning for the Column 'Fatal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Fatal\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Fatal\"] = df[\"Fatal\"].fillna(\"Unknown\").str.strip().astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning for The Column Fatal\n",
    "df[\"Fatal\"] = df[\"Fatal\"].fillna(\"Unknown\")\n",
    "df[\"Fatal\"] = df[\"Fatal\"].str.strip()\n",
    "df[\"Fatal\"].value_counts(dropna=False)\n",
    "df[\"Fatal\"] = df[\"Fatal\"].astype(str)\n",
    "df[\"Fatal\"] = df[\"Fatal\"].replace({\n",
    "    \"Nan\": \"Unknown\",\n",
    "    \" N\": \"NO\",\n",
    "    \"UNKNOWN\": \"Unknown\",\n",
    "    \"F\": \"Unknown\",\\\n",
    "    \"M\": \"NO\",\n",
    "    \"n\": \"NO\",\n",
    "    \"N\": \"NO\",\n",
    "    \"Nq\": \"NO\",\n",
    "    \"Y\": \"YES\",\n",
    "    2017: \"Unknown\",\n",
    "    \"2017 \": \"Unknown\",\n",
    "     \"Y x 2\": \"Yes\",\n",
    "     \"y\": \"Yes\",\n",
    "     \"N   \": \"NO\",\n",
    "     \"nan\": \"Unknown\"\n",
    "})\n",
    "df[\"Fatal\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors and labels dynamically to match data\n",
    "slices = df[\"Fatal\"].value_counts().nlargest(2)\n",
    "labels = slices.index.tolist()  # Get correct labels\n",
    "colors = [\"skyblue\", \"coral\"][:len(labels)]  # Ensure colors match number of slices\n",
    "explode = [0] * len(labels)  # No explode by default\n",
    "\n",
    "# Create pie chart\n",
    "plt.pie(slices, labels=labels, colors=colors, explode=explode, \n",
    "        autopct=\"%1.1f%%\", wedgeprops={\"edgecolor\": \"black\"})\n",
    "\n",
    "plt.title(\"Fatality Rate of Shark Attacks (1967-2025)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"AttacksFatalityRate.png\")\n",
    "\n",
    "# Show the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df[\"Species_Types\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning for the Column 'Country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Country\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[\"Country\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_country = {\n",
    "    'AUSTRALIA': 'Australia',\n",
    "    'BAHAMAS': 'Bahamas',\n",
    "    'BELIZE': 'Belize',\n",
    "    'COLOMBIA': 'Colombia',\n",
    "    'COLUMBIA': 'Colombia',\n",
    "    'EGYPT': 'Egypt',\n",
    "    'ENGLAND': 'United Kingdom',\n",
    "    'FRENCH POLYNESIA': 'French Polynesia',\n",
    "    'INDIA': 'India',\n",
    "    'INDONESIA': 'Indonesia',\n",
    "    'JAMAICA': 'Jamaica',\n",
    "    'JAPAN': 'Japan',\n",
    "    'MALDIVE ISLANDS': 'Maldives',\n",
    "    'MALDIVES': 'Maldives',\n",
    "    'MEXICO': 'Mexico',\n",
    "    'MeXICO': 'Mexico',\n",
    "    'MEXICO ': 'Mexico',\n",
    "    'NEW CALEDONIA': 'New Caledonia',\n",
    "    'NEW ZEALAND': 'New Zealand',\n",
    "    'PHILIPPINES': 'Philippines',\n",
    "    'SPAIN': 'Spain',\n",
    "    'SOUTH AFRICA': 'South Africa',\n",
    "    'South Africa': 'South Africa',\n",
    "    'THAILAND': 'Thailand',\n",
    "    'TRINIDAD': 'Trinidad and Tobago',\n",
    "    'TRINIDAD & TOBAGO': 'Trinidad and Tobago',\n",
    "    'TURKS & CAICOS': 'Turks and Caicos',\n",
    "    'TURKS and CaICOS': 'Turks and Caicos',\n",
    "    'UNITED ARAB EMIRATES (UAE)': 'United Arab Emirates',\n",
    "    'UNITED KINGDOM': 'United Kingdom',\n",
    "    'USA': 'United States',\n",
    "    'UAE': 'United Arab Emirates',\n",
    "    'UNITED STATES': 'United States',\n",
    "    'REUNION ISLAND': 'Reunion',\n",
    "    'REUNION': 'Reunion',\n",
    "    'ST MARTIN': 'Saint Martin',\n",
    "    'ST. MARTIN': 'Saint Martin',\n",
    "    'ST. MAARTIN': 'Saint Martin',\n",
    "    'SAINT MAARTEN': 'Saint Martin',\n",
    "    'PAPUA NEW GUINEA': 'Papua New Guinea',\n",
    "    'FIJI': 'Fiji',\n",
    "    'Fiji': 'Fiji',\n",
    "    'CANADA': 'Canada',\n",
    "    'CUBA': 'Cuba',\n",
    "    'ARGENTINA': 'Argentina',\n",
    "    'BRAZIL': 'Brazil',\n",
    "    'CHILE': 'Chile',\n",
    "    'ECUADOR': 'Ecuador',\n",
    "    'PERU': 'Peru',\n",
    "    'VENEZUELA': 'Venezuela',\n",
    "    'COOK ISLANDS': 'Cook Islands',\n",
    "    'DOMINICAN REPUBLIC': 'Dominican Republic',\n",
    "    'SEYCHELLES': 'Seychelles',\n",
    "    'ST KITTS / NEVIS': 'Saint Kitts and Nevis',\n",
    "    'ST HELENA, British overseas territory': 'Saint Helena',\n",
    "    'SOLOMON ISLANDS': 'Solomon Islands',\n",
    "    'TONGA': 'Tonga',\n",
    "    ' TONGA': 'Tonga',\n",
    "    'KIRIBATI': 'Kiribati',\n",
    "    'PALAU': 'Palau',\n",
    "    'MALTA': 'Malta',\n",
    "    'SCOTLAND': 'United Kingdom',\n",
    "    'IRELAND': 'Ireland',\n",
    "    'ITALY': 'Italy',\n",
    "    'MALAYSIA': 'Malaysia',\n",
    "    'LIBYA': 'Libya',\n",
    "    'JORDAN': 'Jordan',\n",
    "    'ISRAEL': 'Israel',\n",
    "    'CHINA': 'China',\n",
    "    'TAIWAN': 'Taiwan',\n",
    "    'INDIAN OCEAN': 'Indian Ocean',\n",
    "    'INDIAN OCEAN?': 'Indian Ocean',\n",
    "    'INDIAN OCEAN?': 'Indian Ocean',\n",
    "    'NORTH ATLANTIC OCEAN': 'North Atlantic Ocean',\n",
    "    'NORTH ATLANTIC OCEAN ': 'North Atlantic Ocean',\n",
    "    'SOUTH ATLANTIC OCEAN': 'South Atlantic Ocean',\n",
    "    'ATLANTIC OCEAN': 'Atlantic Ocean',\n",
    "    'PACIFIC OCEAN': 'Pacific Ocean',\n",
    "    'PACIFIC OCEAN ': 'Pacific Ocean',\n",
    "    'SOUTH PACIFIC OCEAN': 'South Pacific Ocean',\n",
    "    'NORTH PACIFIC OCEAN': 'North Pacific Ocean',\n",
    "    'CARIBBEAN SEA': 'Caribbean Sea',\n",
    "    'MID ATLANTIC OCEAN': 'Mid Atlantic Ocean',\n",
    "    'SOUTH CHINA SEA': 'South China Sea',\n",
    "    'BAY OF BENGAL': 'Bay of Bengal',\n",
    "    'RED SEA': 'Red Sea',\n",
    "    'RED SEA?': 'Red Sea',\n",
    "    'RED SEA / INDIAN OCEAN': 'Red Sea',\n",
    "    'GULF OF ADEN': 'Gulf of Aden',\n",
    "    'PERSIAN GULF': 'Persian Gulf',\n",
    "    'CEYLON': 'Sri Lanka',\n",
    "    'CEYLON (SRI LANKA)': 'Sri Lanka',\n",
    "    'SRI LANKA': 'Sri Lanka',\n",
    "    'BANGLADESH': 'Bangladesh',\n",
    "    'BURMA': 'Myanmar',\n",
    "    'MYANMAR': 'Myanmar',\n",
    "    'VIETNAM': 'Vietnam',\n",
    "    'HONG KONG': 'Hong Kong',\n",
    "    'MARTINIQUE': 'Martinique',\n",
    "    'NETHERLANDS ANTILLES': 'Netherlands Antilles',\n",
    "    'NORTHERN MARIANA ISLANDS': 'Northern Mariana Islands',\n",
    "    'FEDERATED STATES OF MICRONESIA': 'Micronesia',\n",
    "    'MICRONESIA': 'Micronesia',\n",
    "    'FALKLAND ISLANDS': 'Falkland Islands',\n",
    "    'GIBRALTAR': 'Gibraltar',\n",
    "    'SAUDI ARABIA': 'Saudi Arabia',\n",
    "    'SINGAPORE': 'Singapore',\n",
    "    'SENEGAL': 'Senegal',\n",
    "    'SOMALIA': 'Somalia',\n",
    "    'SOUTH KOREA': 'South Korea',\n",
    "    'NORTH KOREA': 'North Korea',\n",
    "    'RUSSIA': 'Russia',\n",
    "    'GREECE': 'Greece',\n",
    "    'TUNISIA': 'Tunisia',\n",
    "    'TURKEY': 'Turkey',\n",
    "    'IRAN': 'Iran',\n",
    "    'IRAQ': 'Iraq',\n",
    "    'KUWAIT': 'Kuwait',\n",
    "    'LEBANON': 'Lebanon',\n",
    "    'SYRIA': 'Syria',\n",
    "    'AFRICA': 'Africa',\n",
    "    'Coast of AFRICA': 'Africa',\n",
    "    'GEORGIA': 'Georgia',\n",
    "    'GHANA': 'Ghana',\n",
    "    'GUINEA': 'Guinea',\n",
    "    'NAMIBIA': 'Namibia',\n",
    "    'TANZANIA': 'Tanzania',\n",
    "    'ALGERIA': 'Algeria',\n",
    "    'DJIBOUTI': 'Djibouti',\n",
    "    'EQUATORIAL GUINEA / CAMEROON': 'Equatorial Guinea',\n",
    "    'WEST INDIES': 'Caribbean',\n",
    "    'BRITISH ISLES': 'United Kingdom',\n",
    "    'BRITISH WEST INDIES': 'Caribbean',\n",
    "    'ST HELENA, British overseas territory': 'Saint Helena',\n",
    "    'NORWAY': 'Norway',\n",
    "    'ICELAND': 'Iceland',\n",
    "    'GABON': 'Gabon',\n",
    "    'MAYOTTE': 'Mayotte',\n",
    "    'SWEDEN': 'Sweden',\n",
    "    'SLOVENIA': 'Slovenia',\n",
    "    'CURACAO': 'Curaçao',\n",
    "    'HAITI': 'Haiti',\n",
    "    'GUATEMALA': 'Guatemala',\n",
    "    'NICARAGUA': 'Nicaragua',\n",
    "    'NICARAGUA ': 'Nicaragua',\n",
    "    'HONDURAS': 'Honduras',\n",
    "    'EL SALVADOR': 'El Salvador',\n",
    "    'COSTA RICA': 'Costa Rica',\n",
    "    'PANAMA': 'Panama',\n",
    "    'BARBADOS': 'Barbados',\n",
    "    'ARUBA': 'Aruba',\n",
    "    'GRAND CAYMAN': 'Cayman Islands',\n",
    "    'CAYMAN ISLANDS': 'Cayman Islands',\n",
    "    'SAINT LUCIA': 'Saint Lucia',\n",
    "    'USA': 'United States', \n",
    "    'Usa': 'United States'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'] = df['Country'].replace(update_country).fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Data Cleaning for the column 'Country', I've tried to use regex \n",
    "df[\"Country\"] = df[\"Country\"].str.strip().str.title()\n",
    "df[\"Country\"] = df[\"Country\"].str.replace(r\"[^a-zA-Z\\s]\", \"\", regex=True)\n",
    "#df[\"Country\"] = df[\"Country\"].map(lambda x: x.upper())\n",
    "df[\"Country\"] = df[\"Country\"].fillna(\"Unknown\")\n",
    "print(df[\"Country\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_country = len(df['Country'].unique())\n",
    "unique_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Country\"] = df[\"Country\"].replace(update_country) # Reassigning it back to \"Country\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Country\n",
    "top_10_attacks_country = df[\"Country\"].value_counts().nlargest(9)\n",
    "\n",
    "# Graphic Attacks by State \n",
    "top_10_attacks_country.sort_values(ascending=True, inplace=True)\n",
    "top_10_attacks_country.plot.barh(color=\"skyblue\")\n",
    "\n",
    "plt.title(\"Shark Attacks in Countries (1967 -2017)\")\n",
    "plt.xlabel(\"Attacks\")\n",
    "plt.grid(axis=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"AttacksByCountry.png\") # Saving as an image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning for the Column 'States'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A filter specifically for USA \n",
    "filter_usa = (df[\"Country\"] == \"United States\")\n",
    "usa_df = df.loc[filter_usa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Shark Attacks by State\n",
    "top_attacks_usa = df[\"State\"].value_counts()\n",
    "top_attacks_usa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Shark Attacks by State\n",
    "top_10_attacks_usa = df[\"State\"].value_counts().nlargest(9)\n",
    "\n",
    "# Graphic Attacks by State \n",
    "top_10_attacks_usa.sort_values(ascending=True, inplace=True)\n",
    "top_10_attacks_usa.plot.barh(color=\"skyblue\")\n",
    "\n",
    "plt.title(\"Shark Attacks in the U.S by State (1967 -2017)\")\n",
    "plt.xlabel(\"Attacks\")\n",
    "plt.grid(axis=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"AttacksByState.png\") # Saving as an image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning for the Column 'Age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Age\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Age\"] = df[\"Age\"].str.strip()\n",
    "print(df[\"Age\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Age\"] = df[\"Age\"].replace({\"20s\":\"25\", \"30s\":\"35\", \"teen\": \"15\", \"Teen\": \"15\", \"mid-30s\":\"35\", \"21 or 26\":\"24\", \"60's\":\"65\", \"60s\":\"65\", \"12 or 13\":\"12\", \"50s\":\"55\", \"Middle age\": \"40\", \"9 & 12\":\"9\", \"Elderly\": \"12\", \"6½\":\"6\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Age\"].unique()[:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_process(age):\n",
    "    try:\n",
    "        age = int(age)\n",
    "    except:\n",
    "        age = 0\n",
    "    if (age > 0 and age <= 100):\n",
    "        return age\n",
    "    else:\n",
    "        return np.nan\n",
    "df['Age'].fillna(0,inplace=True)\n",
    "df['Age'] = df['Age'].apply(age_process)\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "ax = sns.distplot(df['Age'].dropna().astype(np.int32),\n",
    "             ax=ax,\n",
    "             hist_kws={\"alpha\": 0.6, \"color\": \"skyblue\"},\n",
    "             kde=False,bins=15)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Age Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphic Attacks By Age\n",
    "\n",
    "usa_df[\"Age\"] = pd.to_numeric(usa_df[\"Age\"], errors=\"coerce\")\n",
    "\n",
    "age_usa = usa_df[\"Age\"].dropna()\n",
    "\n",
    "# Making bins for age groups\n",
    "bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 100]\n",
    "plt.hist(age_usa, bins=bins, edgecolor = \"black\", color=\"skyblue\")\n",
    "\n",
    "median = age_usa.median()\n",
    "plt.axvline(median, color=\"coral\", label= f\"Median Age: {median:.1f}\", linewidth=2)\n",
    "\n",
    "# Labels and Title \n",
    "plt.title(\"Ages of Shark Attack VIctims in the USA (1960-2025)\")\n",
    "plt.ylabel(\"Attacks\")\n",
    "plt.xlabel(\"Age\")\n",
    "\n",
    "# Formatting\n",
    "plt.legend()\n",
    "plt.grid(axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"AttacksByAge.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning for the Column 'Location'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Location\"] = df[\"Location\"].astype(str).str.strip()\n",
    "df[\"Location\"] = df[\"Location\"].map(lambda x:x.upper())\n",
    "df[\"Location\"] = df[\"Location\"].fillna(\"Unknown\")\n",
    "#df[\"Location\"] = df[\"Location\"]\n",
    "df[\"Location\"] = df[\"Location\"].replace({ \"NAN\": \"Unknown\", \"nan\": \"Unknown\",\"Nan\": \"Unknown\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Location\"].unique()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Location\"] = df[\"Location\"].str.strip().str.lower().str.capitalize()\n",
    "df[\"Location\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphic Attacks by Year \n",
    "world_attacks_year = df[\"Year\"].value_counts()\n",
    "world_attacks_year.sort_index(ascending=True, inplace=True)\n",
    "world_attacks_year.plot(color=\"skyblue\", label =\"Worldwide\")\n",
    "\n",
    "usa_attacks_year = usa_df[\"Year\"].value_counts()\n",
    "usa_attacks_year.sort_index(ascending=True, inplace=True)\n",
    "usa_attacks_year.plot(color=\"coral\", label =\"United States\")\n",
    "\n",
    "x_years = df[\"Year\"].value_counts().sort_index().index\n",
    "\n",
    "plt.fill_between(x_years, usa_attacks_year, world_attacks_year, color=\"skyblue\", alpha=0.25)\n",
    "plt.fill_between(x_years, usa_attacks_year, color=\"coral\", alpha=0.25)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"AttacksByYear.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
